# -*- coding: utf-8 -*-
"""Titanik.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A1uLzhBHQqApAmLgDBFOMsquYekOGYF-
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# train test ga ajratish uchun kerak bo'ladi
from sklearn.model_selection import train_test_split

# metrics
from sklearn.metrics import  accuracy_score, recall_score, precision_score, f1_score, roc_auc_score,jaccard_score
from sklearn.model_selection import cross_val_score

# models 
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.ensemble import BaggingClassifier

"""### outliyer qiymatlarni topish"""

# def outliyer(data,*args):
#   for cl in args:
#     min_out = data[cl].describe()[4] - (data[cl].describe()[5] * 1.5)
#     max_out = data[cl].describe()[6] + (data[cl].describe()[5] * 1.5)

#     data[cl] = data[cl].apply(lambda a: np.nan if a > max_out or a < min_out else a)
#     print(min_out,max_out)
#   return data

df=pd.read_csv("/content/titanic.csv")
df.head(10)

df.info()

df.isnull().sum()

df.describe()

df.drop(columns=['PassengerId','Cabin'],inplace=True)

Last_name=[]
First_name=[]
for i in df['Name']:
  L,N=i.split(',')
  Last_name.append(L)
  First_name.append(N)
print(First_name)

df['Last_name']=pd.Series(Last_name)
df['First_name']=pd.Series(First_name)
df=df.drop(columns=['Name'])

son=[]
xarf=[]
for i in df['Ticket']:
  if len(i.split())==2:
    x,s=i.split()
    xarf.append(x)
    son.append(s)
  if len(i.split())==1:
    if str(i).isdigit():
      son.append(i)
    else:
      i=np.nan
      son.append(i)

df['Ticket']=pd.Series(son)

df['Ticket'].isnull().sum()

df

df['Ticket']=df['Ticket'].astype('float64')

df['Ticket']=df['Ticket'].map(lambda a:np.nan if a<1000 else a )

df['Ticket'].isnull().sum()

df['Ticket'].fillna(df['Ticket'].mean(),inplace=True)

df['Ticket'].isnull().sum()

df

df.describe()

df['Age']=df['Age'].fillna(df['Age'].mean())

df.isnull().sum()

df['Embarked'].fillna(method='ffill',inplace=True)

df.head(10)

df['Sex'].value_counts()

#df['Sex']=df['Sex'].map(lambda a: 0 if a=='male' else 1)

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

col=['Embarked','Last_name','First_name','Sex']
for j in col:
  df[j]=le.fit_transform(df[j])

df=df.astype('float64')

df.info()

"""### MA'LUMOTLAR HAQIDA VIZUAL FIKRLAYMIZ"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
df.hist(bins=50, figsize=(20,15))
plt.show()

"""###  biz malumotlarimiz bir biriga qanchalik bog'langanligini vizual tarzda ko'rishimiz mumkin"""

corr_matrix = df.corr().abs()
corr_matrix.style.background_gradient(cmap='coolwarm')

"""### MA'LUMOTLAR QANCHALI TARGETGA BOG'LIQ BO'LSA BIZDA NATIJA SHUNCHA YAXSHI CHIQADI !"""

df.corrwith(df['Survived']).abs().sort_values(ascending=False)

"""TARGETNI AJRATIB OLISH"""

x=df.drop(columns='Survived')
y=df['Survived']



Surv_rate = y.value_counts()/len(df)*100
plt.figure(figsize=(5,5))
plt.pie(Surv_rate, labels=['T_Qolgan',"O'lgan"])
plt.show()

"""MALUMOTLARNI SCALE QILISH"""

from sklearn.preprocessing import MinMaxScaler
mc=MinMaxScaler()
scaled_df=mc.fit_transform(x)

x=pd.DataFrame(data=scaled_df,index=x.index,columns=x.columns)



"""TRAIN, TESTGA ajratish """

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=0)

x_test

"""MANA BO'LDI MALUMOTLARIMIZ  "ML"   UCHUN TAYYOR ANA ENDI MODELLARNI BIRMA BIR ISHLATIB KO'RAMIZ

### Logistic Regression
"""

from sklearn import metrics
# Modelni yaratamiz (training)
LR_model = LogisticRegression()
LR_model.fit(x_train, y_train)

# Modelni baholaymiz
y_pred = LR_model.predict(x_test)
print(metrics.classification_report(y_test, y_pred))
print("Model aniqligi:", metrics.accuracy_score(y_test,y_pred))

## confusion matrix
conf_mat = metrics.confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True,fmt="g")
plt.show()

## ROC curve
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC curve')
display.plot()
plt.show()

"""### Support Vector Machines"""

# Modelni yaratamiz (training)
svm_model = SVC()
svm_model.fit(x_train, y_train)

# Modelni baholaymiz
y_pred = svm_model.predict(x_test)
print(metrics.classification_report(y_test, y_pred))
print("Model aniqligi:", accuracy_score(y_test,y_pred))

## confusion matrix
conf_mat = metrics.confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True,fmt="g")
plt.show()

## ROC curve
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC curve')
display.plot()
plt.show()

"""### Decision Tree"""

# Modelni yaratamiz (training)
tree_model = DecisionTreeClassifier()
tree_model.fit(x_train, y_train)

# Modelni baholaymiz
y_pred = tree_model.predict(x_test)
print(metrics.confusion_matrix(y_test, y_pred))
print("Model aniqligi:", accuracy_score(y_test,y_pred))

## confusion matrix
conf_mat = metrics.confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True,fmt="g")
plt.show()

## ROC curve
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC curve')
display.plot()
plt.show()

"""### Random Forest"""

# Modelni yaratamiz (training)
RF_model = RandomForestClassifier(n_estimators=9)
RF_model.fit(x_train, y_train)

# Modelni baholaymiz
y_pred = RF_model.predict(x_test)
print(metrics.classification_report(y_test, y_pred))
print("Model aniqligi:", accuracy_score(y_test,y_pred))

## confusion matrix
conf_mat = metrics.confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True,fmt="g")
plt.show()

## ROC curve
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC curve')
display.plot()
plt.show()

"""### XGBOOST"""

from xgboost import XGBClassifier
# Modelni yaratamiz (training)
xgb_model = XGBClassifier()
xgb_model.fit(x_train, y_train)

# Modelni baholaymiz
y_pred = xgb_model.predict(x_test)
print(metrics.classification_report(y_test, y_pred))
print("Model aniqligi:", accuracy_score(y_test,y_pred))

## confusion matrix
conf_mat = metrics.confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True,fmt="g")
plt.show()

## ROC curve
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC curve')
display.plot()
plt.show()

"""### RANDOMFOREST MODEL --> bizda yaxshi chiqdi va biz bu modelni "Hyperparameter" lari yordamida optimallashtirishga harakat qilamiz"""

rf = RandomForestClassifier()
rf.fit(x_train,y_train)
y_pred5 = rf.predict(x_test)

"""### optimal Hyperparameter larini qidirish"""

from sklearn.model_selection import RandomizedSearchCV

random_grid = {'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}

hyper_tuning= RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)

#hyper_tuning.fit(x_train,y_train)

#hyper_tuning.best_params_

"""### mana ularni topdik va endi ularni sinap ko'ramiz"""

# rf = RandomForestClassifier(n_estimators=2000,min_samples_split=10,min_samples_leaf=1,max_features='auto',max_depth=60,bootstrap=True)
# rf.fit(x_train,y_train)
# y_pred5 = rf.predict(x_test)

#accuracy_score(y_pred5,y_test)

"""**### Ko'rip turganingizday bizda natija Hyperparameter by default  xolatiga qaraganda pastroq chiqdi....**"""



"""# Modelni saqlab olamiz pickle yordamida"""

import pickle

filename = 'xgb_model' # faylga istalgan nom beramiz
with open(filename, 'wb') as file:
    pickle.dump(xgb_model, file)

# import pickle

# filename = 'model.pkl' # faylga istalgan nom beramiz
# with open(filename, 'wb') as file:
#     pickle.dump(LR_model, file)

"""# Modelni qayta o'qiymiz: """

with open(filename, 'rb') as file:
  model = pickle.load(file)

# scores = cross_val_score(model, x, y, scoring="neg_mean_squared_error", cv=5)
# LR_rmse_scores = np.sqrt(-scores)
# display_scores(LR_rmse_scores)

x_train

print(x_train.iloc[0])

x.iloc[0]